from openai import OpenAI
import streamlit as st
import pandas as pd
import numpy as np
import os
from dotenv import load_dotenv
import datetime
from langchain_community.document_loaders.csv_loader import CSVLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.vectorstores.base import VectorStore
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
import re

# openai APIí‚¤ ì…ë ¥
load_dotenv()

client = OpenAI(
    api_key = os.getenv("OPENAI_API_KEY")
)

model = ChatOpenAI(model="gpt-4o-mini")

# csv íŒŒì¼ ë¡œë“œ.
loader_1 = CSVLoader('sum_sum_Cultural2.csv',encoding='UTF8')
loader_2 = CSVLoader('place.csv',encoding='UTF8')

Tresures = loader_1.load()
Travel = loader_2.load()

recursive_text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=100,
    chunk_overlap=10,
    length_function=len,
    is_separator_regex=False,
)

splits = recursive_text_splitter.split_documents(Tresures)
splits += recursive_text_splitter.split_documents(Travel)


embeddings = OpenAIEmbeddings(model="text-embedding-ada-002")

if os.path.exists('./db/faiss'):
    vectorstore = FAISS.load_local('./db/faiss', embeddings, allow_dangerous_deserialization=True)
else:
    vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)
    vectorstore.save_local('./db/faiss')

retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 1})


# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
contextual_prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant. Please provide detailed, relevant recommendations based on the context provided. Ensure that all recommendations share the same city, district, and road name, and respond only with information from the data provided. Please recommend places in the same city and district. If there are no places in the same district, recommend places in the same city. Be specific and clear in your recommendations Answer in Korean."),
    ("user", "Context: {context}\\n\\nQuestion: {question}. Provide clear and specific recommendations based on the data.")

    # Few-shot ì˜ˆì‹œ
    #("system", "ì˜ˆì‹œ 1: \nContext: ìˆ­ë¡€ë¬¸ì€ ì„œìš¸ì‹œ ì¤‘êµ¬ ì„¸ì¢…ëŒ€ë¡œ 40(ë‚¨ëŒ€ë¬¸ë¡œ4ê°€)ì— ìœ„ì¹˜í•˜ê³  ìˆìŠµë‹ˆë‹¤.\\nQuestion: ìˆ­ë¡€ë¬¸ì˜ ê·¼ë°© ì¶”ì²œí•´ì¤˜.\\nAnswer: ì„œìš¸ì‹œ ì¤‘êµ¬ ê·¼ì²˜ì—ëŠ” ì¤‘êµ¬ë¬¸í™”ì›, ëª©ë©±ì‚°ë´‰ìˆ˜ëŒ€í„°, ì´ì¶©ìˆœ ìê²° í„° ë“±ì´ ìˆìŠµë‹ˆë‹¤."),
    #("system", "ì˜ˆì‹œ 2: \nContext: ê²½ë³µê¶ì€ ì„œìš¸ì‹œ ì¢…ë¡œêµ¬ ì‚¬ì§ë™ì— ìœ„ì¹˜í•´ ìˆìŠµë‹ˆë‹¤.\\nQuestion: ê²½ë³µê¶ì˜ ê·¼ë°© ì¶”ì²œí•´ì¤˜.\\nAnswer: ì„œìš¸ ì¢…ë¡œêµ¬ ì‚¬ì§ë™ ê·¼ì²˜ì—ëŠ” ë¶ì´Œí•œì˜¥ë§ˆì„, ì²­ì™€ëŒ€, ì°½ë•ê¶ ë“±ì´ ìˆìŠµë‹ˆë‹¤."),
    #("system", "ì˜ˆì‹œ 3: \nContext: ë¶ˆêµ­ì‚¬ëŠ” ê²½ìƒë¶ë„ ê²½ì£¼ì‹œì— ìœ„ì¹˜í•˜ê³  ìˆìŠµë‹ˆë‹¤.\\nQuestion: ë¶ˆêµ­ì‚¬ì˜ ê·¼ë°© ì¶”ì²œí•´ì¤˜.\\nAnswer: ê²½ìƒë¶ë„ ê²½ì£¼ ê·¼ì²˜ì—ëŠ” ì„êµ´ì•”, ê²½ì£¼ êµ­ë¦½ë°•ë¬¼ê´€, ì•ˆì••ì§€ ë“±ì´ ìˆìŠµë‹ˆë‹¤."),
    #("system", "ì˜ˆì‹œ 4: \nContext: í‰ì°½ ì›”ì •ì‚¬ ì„ì¡°ë³´ì‚´ì¢ŒìƒëŠ” ê°•ì› í‰ì°½êµ° ì§„ë¶€ë©´ ì˜¤ëŒ€ì‚°ë¡œì— ìœ„ì¹˜í•˜ê³  ìˆìŠµë‹ˆë‹¤.\\nQuestion: í‰ì°½ ì›”ì •ì‚¬ ì„ì¡°ë³´ì‚´ì¢Œìƒì˜ ê·¼ë°© ì¶”ì²œí•´ì¤˜.\\nAnswer: ê°•ì› í‰ì°½êµ° ê·¼ì²˜ì—ëŠ” ì•Œíœì‹œì•„ ì•ŒíŒŒì¸ì½”ìŠ¤í„°, ìœ¡ì‹­ë§ˆì§€ê¸°, ë°±ë£¡ë™êµ´ ë“±ì´ ìˆìŠµë‹ˆë‹¤."),
    #("system", "ì˜ˆì‹œ 5: \nContext: ë‚¨ì› ìš©ë‹´ì‚¬ì§€ ì„ì¡°ì—¬ë˜ì…ìƒëŠ” ì „ë¶ ë‚¨ì›ì‹œ ì£¼ì²œë©´ ì›ì²œë¡œì— ìœ„ì¹˜í•˜ê³  ìˆìŠµë‹ˆë‹¤.\\nQuestion: ë‚¨ì› ìš©ë‹´ì‚¬ì§€ ì„ì¡°ì—¬ë˜ì…ìƒì˜ ê·¼ë°© ì¶”ì²œí•´ì¤˜.\\nAnswer: ì „ë¶ ë‚¨ì›ì‹œ ê·¼ì²˜ì—ëŠ” ì§€ë¦¬ì‚° í—ˆë¸Œë°¸ë¦¬, ì¶˜í–¥í…Œë§ˆíŒŒí¬, í•­ê³µìš°ì£¼ì²œë‚¨ì›ë¬¸ëŒ€ ë“±ì´ ìˆìŠµë‹ˆë‹¤."),
    #("system", "ì˜ˆì‹œ 6: \nContext: ì´ìˆœì‹  ë¬´ê³¼í™íŒ¨ëŠ” ì¶©ë‚¨ ì•„ì‚°ì‹œ ì—¼ì¹˜ì í˜„ì¶©ì‚¬ê¸¸ 130 í˜„ì¶©ì‚¬ê´€ë¦¬ì†Œì— ìœ„ì¹˜í•˜ê³  ìˆìŠµë‹ˆë‹¤.\\nQuestion: ì´ìˆœì‹  ë¬´ê³¼í™íŒ¨ì˜ ê·¼ë°© ì¶”ì²œí•´ì¤˜.\\nAnswer: ì¶©ë‚¨ ì•„ì‚°ì‹œ ê·¼ì²˜ì—ëŠ” í”¼ë‚˜í´ëœë“œ ìˆ˜ëª©ì›, ê³¡êµì²œ ì€í–‰ë‚˜ë¬´ê¸¸, ì„¸ê³„ê½ƒì‹ë¬¼ì› ë“±ì´ ìˆìŠµë‹ˆë‹¤."),
])




# ë””ë²„ê¹…ì„ ìœ„í•´ ë§Œë“  í´ë˜ìŠ¤ (ì‹ ê²½ì“°ì§€ ì•Šìœ¼ì…”ë„ ë©ë‹ˆë‹¤.)
class SimplePassThrough:
    def invoke(self, inputs, **kwargs):
        return inputs

# í”„ë¡¬í”„íŠ¸ í´ë˜ìŠ¤
class ContextToPrompt:
    def __init__(self, prompt_template):
        self.prompt_template = prompt_template

    def invoke(self, inputs):
        # response_docs ë‚´ìš©ì„ trimí•´ì¤Œ (ê°€ë…ì„±ì„ ë†’ì—¬ì¤Œ)
        if isinstance(inputs, list): # inputsê°€ listì¸ ê²½ìš°. ì¦‰ ì—¬ëŸ¬ê°œì˜ ë¬¸ì„œë“¤ì´ ê²€ìƒ‰ë˜ì–´ ë¦¬ìŠ¤íŠ¸ë¡œ ì „ë‹¬ëœ ê²½ìš°
            context_text = "\n".join([doc.page_content for doc in inputs]) # \nì„ êµ¬ë¶„ìë¡œ ë„£ì–´ì„œ í•œ ë¬¸ìì—´ë¡œ í•©ì³ì¤Œ
        else:
            context_text = inputs # ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹Œê²½ìš°ëŠ” ê·¸ëƒ¥ ë¦¬í„´í•´ì¤Œ

        # í”„ë¡¬í”„íŠ¸
        formatted_prompt = self.prompt_template.format_messages( # í…œí”Œë¦¿ì˜ ë³€ìˆ˜ì— ì‚½ì…í•´ì¤Œ
            context=context_text, # {context} ë³€ìˆ˜ì— context_text, ì¦‰ ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©ì„ ì‚½ì…í•¨
            question=inputs.get("question", "")
        )
        return formatted_prompt

# Retriever í´ë˜ìŠ¤
class RetrieverWrapper:
    def __init__(self, retriever):
        self.retriever = retriever

    def invoke(self, inputs):
        # 0ë‹¨ê³„ : queryì˜ íƒ€ì…ì— ë”°ë¥¸ ì „ì²˜ë¦¬
        if isinstance(inputs, dict): # inputsê°€ ë”•ì…”ë„ˆë¦¬ íƒ€ì…ì¼ê²½ìš°, question í‚¤ì˜ ê°’ì„ ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ ì‚¬ìš©
            query = inputs.get("question", "")
        else: # ì§ˆë¬¸ì´ ë¬¸ìì—´ë¡œ ì£¼ì–´ì§€ë©´, ê·¸ëŒ€ë¡œ ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ ì‚¬ìš©
            query = inputs
        # 1ë‹¨ê³„ : queryë¥¼ ë¦¬íŠ¸ë¦¬ë²„ì— ë„£ì–´ì£¼ê³ , response_docsë¥¼ ì–»ì–´ëª¨
        response_docs = self.retriever.get_relevant_documents(query) # ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê³  ê²€ìƒ‰ ê²°ê³¼ë¥¼ response_docsì— ì €ì¥
        return response_docs

# RAG ì²´ì¸ ì„¤ì •
rag_chain_debug = {
    "context": RetrieverWrapper(retriever), # í´ë˜ìŠ¤ ê°ì²´ë¥¼ ìƒì„±í•´ì„œ valueë¡œ ë„£ì–´ì¤Œ
    "prompt": ContextToPrompt(contextual_prompt),
    "llm": model
}


# ì‚¬ì´ë“œ ë°” ì„¤ì •

# ì§€ë„ ì„¤ì •
data = pd.DataFrame({
    'lat':[37.56],
    'lon':[127],
	})

with st.sidebar.expander("ì§€ë„ í¼ì¹˜ê¸°"):
    st.map(data,latitude='lat', longitude='lon')


# ë‚ ì§œ
# ì˜¤ëŠ˜ ë‚ ì§œ ìë™ ì„¤ì •
today = datetime.date.today()

# ì‚¬ì´ë“œë°”ì— ë‚ ì§œ ì…ë ¥ ìœ„ì ¯ì„ ì¶”ê°€í•˜ê³ , ì˜¤ëŠ˜ ë‚ ì§œë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •
st.sidebar.title("ë‹¬ë ¥")
travel_date = st.sidebar.date_input("ë‚ ì§œë¥¼ í´ë¦­ í•˜ì„¸ìš”", today)

# ì„ íƒëœ ë‚ ì§œ ì¶œë ¥í• ê»€ê°€ìš”? ì¼ë‹¨ ì ì–´ë‘ê² ìŠµë‹ˆë‹¤
# st.write(f"ì„ íƒí•œ ë‚ ì§œ: {travel_date}")

    
import time



# ì‹¤ì‹œê°„ ì§€ì—­ë³„ ì˜¨ë„ ì—…ë°ì´íŠ¸(ë‰´ìŠ¤ì²˜ëŸ¼)
#import requests
# ì¤‘ìš”!!!! OpenWeatherMap API ê°€ì…í•˜ê³  ê°€ì ¸ì™€ì•¼í•¨ ê·¸ë˜ì„œ ì“¸ê»€ì§€,ë§ê»€ì§€ ë“± ì—¬ì­¤ë³´ë ¤í•©ë‹ˆë‹¤
# API_KEY ìœ„ì—ì„œ ë¶€ë¦„
#def get_current_temperature(region):
#    url = f"http://api.openweathermap.org/data/2.5/weather?q={region},kr&appid={API_KEY}&units=metric"
#    response = requests.get(url)
#    data = response.json()
#    return data["main"]["temp"]  # ì„­ì”¨ ì˜¨ë„ë¥¼ ë°˜í™˜

# ì§€ì—­ ë¦¬ìŠ¤íŠ¸
#regions = ["Seoul", "Busan", "Daegu", "Incheon", "Gwangju", "Daejeon", "Ulsan", "Gangneung", "Jeju"]

# ë¹„ì–´ìˆëŠ” ì»¨í…Œì´ë„ˆ ìƒì„±
#metric_containers = {region: st.empty() for region in regions}

# ì‹¤ì‹œê°„ ì˜¨ë„ ì—…ë°ì´íŠ¸
#for _ in range(100):
#    for region in regions:
#        current_temperature = get_current_temperature(region)
#        delta = random.choice([1, -1])  # ì˜¨ë„ì˜ ë³€í™”ëŸ‰
#        new_temperature = current_temperature + delta
#       
#        # ê° ì§€ì—­ì˜ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
#        metric_containers[region].metric(
#            label=f"{region} í˜„ì¬ ì˜¨ë„",
#            value=f"{new_temperature}Â°C",
#            delta=f"{delta}Â°C"
#        )   
#    # 5ì´ˆ ëŒ€ê¸°
#    time.sleep(5)




# ì§„í–‰ë¥ ì´ ëë‚œ í›„ íƒ€ì´í‹€ í‘œì‹œ, íƒ€ì´í‹€ ì‚¬ìš© ì‹œ ì´ê±´ ë­˜ ì‚¬ìš©í•  ì§€ ì •í•´ì•¼í•  ê²ƒ ê°™ìŠµë‹ˆë‹¤~~~
st.title("77M PROJECTğŸ¤–")
st.image("ê²½ë³µê¶.jpg", caption="")

# ê³µì§€ê¸€ í‘œì‹œ (íˆ¬ëª…í•œ ë°°ê²½ê³¼ ì£¼í™©ìƒ‰ ê¸€ì”¨)
st.markdown("""
    <div style="background-color: rgba(255, 255, 255, 0.5); padding: 10px; border-radius: 5px;">
        <strong style="color: orange;">â–¶</strong> ë¬¸í™”ì¬ ì£¼ë³€ì˜ ì—¬í–‰ì§€ ì¶”ì²œ ì±—ë´‡ì…ë‹ˆë‹¤!<br>  
        ëŒ€í•œë¯¼êµ­ì˜ ê¶ê¸ˆí•œ êµ­ë³´ë‚˜ ë³´ë¬¼ì„ ì…ë ¥í•˜ë©´ ì£¼ë³€ ì—¬í–‰ì§€ë¥¼ ì¶”ì²œí•´ë“œë¦½ë‹ˆë‹¤.<br>  
        " 000ì˜ ê·¼ë°© ì¶”ì²œí•´ì¤˜ "ë¼ê³  ì…ë ¥í•´ì£¼ì„¸ìš”!
    </div>
""", unsafe_allow_html=True)




# ëŒ€í™” ê¸°ë¡ì„ í‚¤ì›Œë“œë³„ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜
def store_conversation(keyword, message):
    if keyword not in st.session_state.messages:
        st.session_state.messages[keyword] = []
    st.session_state.messages[keyword].append(message)

# ì„¸ì…˜ ìƒíƒœì— ë©”ì‹œì§€ ê¸°ë¡ì´ ì—†ìœ¼ë©´ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = {}



# ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì§ˆë¬¸ ì²˜ë¦¬
if query := st.chat_input("ëŒ€í•œë¯¼êµ­ì˜ êµ­ë³´ ë˜ëŠ” ë³´ë¬¼ì„ ì…ë ¥í•˜ì‹œê±°ë‚˜ ë¬¼ì–´ë³´ì„¸ìš”"):
    # ì‚¬ìš©ì ì…ë ¥ í‘œì‹œ
    with st.chat_message("user"):
        st.markdown(query)

    # í‚¤ì›Œë“œ ê¸°ë³¸ê°’ ì„¤ì •
    keyword = None

    # ì‚¬ìš©ìì˜ ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œë¥¼ ê°ì§€
    match = re.match(r"(\S+)\s*ì˜\s*ê·¼ë°©\s*ì¶”ì²œí•´ì¤˜", query)
    if match:
        # í‚¤ì›Œë“œ ì¶”ì¶œ
        keyword = match.group(1)

    # í‚¤ì›Œë“œê°€ ì¶”ì¶œëœ ê²½ìš°ë§Œ ëŒ€í™” ê¸°ë¡ ì €ì¥
    if keyword:
        store_conversation(keyword, {"role": "user", "content": query})

        # 1. ë¦¬íŠ¸ë¦¬ë²„ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ ì €ì¥
        response_docs = rag_chain_debug["context"].invoke({"question": query})

        # 2. í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt_messages = rag_chain_debug["prompt"].invoke({
            "context": response_docs,
            "question": query
        })

        # 3. ì‘ë‹µ ìƒì„±
        response = rag_chain_debug["llm"].invoke(prompt_messages)

        # ì‘ë‹µ ì €ì¥
        store_conversation(keyword, {"role": "assistant", "content": response.content})

        # ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ ì¶œë ¥
        with st.chat_message("assistant"):
            if response and hasattr(response, 'content') and response.content:
                st.markdown(response.content)
            else:
                st.markdown("ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    else:
        # í‚¤ì›Œë“œê°€ ì—†ì„ ê²½ìš° ì˜¤ë¥˜ ì²˜ë¦¬
        with st.chat_message("assistant"):
            st.markdown("ì…ë ¥ì—ì„œ í‚¤ì›Œë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. '000ì˜ ê·¼ë°© ì¶”ì²œí•´ì¤˜' í˜•ì‹ìœ¼ë¡œ ì…ë ¥í•´ì£¼ì„¸ìš”.")

# ì‚¬ì´ë“œë°”ì— í´ë¦­ ê°€ëŠ¥í•œ í‚¤ì›Œë“œë¥¼ í‘œì‹œ(ëŒ€í™” ë‚´ìš©ì— ë§ì¶°ì„œ)
st.sidebar.title("ëŒ€í™” ê¸°ë¡")
for keyword in st.session_state.messages.keys():
    if st.sidebar.button(keyword):
        # í‚¤ì›Œë“œë¥¼ í´ë¦­í•˜ë©´ í•´ë‹¹ í‚¤ì›Œë“œì— ê´€ë ¨ëœ ëŒ€í™” ë‚´ì—­ì„ í‘œì‹œ
            for msg in st.session_state.messages[keyword]:
                st.sidebar.markdown(f"**{msg['role']}**: {msg['content']}")












  
    
    
    
    
