import os
import streamlit as st
import pandas as pd
import datetime
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain.document_loaders import CSVLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import LLMChain
from langchain_core.prompts import ChatPromptTemplate

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# OpenAI API key ì„¤ì •
api_key = os.getenv("OPENAI_API_KEY")
model = ChatOpenAI(model="gpt-4o-mini")

# ë¬¸ì„œ ë¡œë“œ ë° ì²˜ë¦¬
loader1 = CSVLoader("sum_sum_Cultural2.csv", encoding='UTF8')
loader2 = CSVLoader("place.csv", encoding='UTF8')

Treasures = loader1.load()
Travel = loader2.load()

text_splitter = CharacterTextSplitter(separator="\n", chunk_overlap=10, length_function=len)
splits = text_splitter.split_documents(Treasures)
splits2 = text_splitter.split_documents(Travel)


# ë‘ ê°œì˜ splits ë¦¬ìŠ¤íŠ¸ë¥¼ í•©ì¹˜ê¸°(ì²­í¬ ë”ì´ìƒxx)
combined_splits = splits + splits2

# ë²¡í„°í™” ë° ìŠ¤í† ì–´ ìƒì„±
embeddings = OpenAIEmbeddings(model="text-embedding-ada-002")
vectorstore = FAISS.from_documents(documents=combined_splits, embedding=embeddings)
retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 1})

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •
contextual_prompt = ChatPromptTemplate.from_messages([
    ("system", "Answer the question using only the following context."),
    ("user", "Context: {context}\\n\\nQuestion: {question}")
])

# LLMChain ì„¤ì •
llm_chain = LLMChain(llm=model, prompt=contextual_prompt)

# Streamlit UI ì„¤ì •
st.title("77MğŸ¤–")

# ì‚¬ìš©ì ì…ë ¥
prompt = st.chat_input("ëŒ€í•œë¯¼êµ­ì˜ êµ­ë³´ ë˜ëŠ” ë³´ë¬¼ì„ ì…ë ¥í•˜ì‹œê±°ë‚˜ ë¬¼ì–´ë³´ì„¸ìš”")

# ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ OpenAI ì‘ë‹µ ìƒì„±
def openai_stream(question):
    try:
        # ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±
        response_Treasures = retriever.get_relevant_documents(question)
        context_text = "\n".join([doc.page_content for doc in response_Treasures])
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì— ë°ì´í„° ì ìš©
        prompt_messages = contextual_prompt.format_messages(
            context=context_text,
            question=question
        )
        
        # LLMìœ¼ë¡œ ì‘ë‹µ ìƒì„±
        response = llm_chain.invoke(prompt_messages)
        return response.content
    except Exception as e:
        st.error(f"Error: {str(e)}")

# ì‚¬ìš©ì ì§ˆë¬¸ ì²˜ë¦¬
if prompt:
    if "messages" not in st.session_state:
        st.session_state.messages = []
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})    

    with st.chat_message("assistant"):
        response = openai_stream(prompt)
        st.markdown(response)  # ì‘ë‹µ ì¶œë ¥
    st.session_state.messages.append({"role": "assistant", "content": response})

# ì‚¬ì´ë“œë°” ì„¤ì •: ì§€ë„ ë° ë‚ ì§œ
data = pd.DataFrame({'lat': [37.56], 'lon': [127]})
st.sidebar.title("ì§€ë„")
st.sidebar.map(data, latitude='lat', longitude='lon')

st.sidebar.title("ë‹¬ë ¥")
travel_date = st.sidebar.date_input("ë‚ ì§œë¥¼ í´ë¦­ í•˜ì„¸ìš”", datetime.date(2024, 12, 1))
